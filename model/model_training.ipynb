{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obesity Level Classification - Model Training\n",
    "\n",
    "**Dataset**: Estimation of Obesity Levels Based on Eating Habits and Physical Condition (UCI)\n",
    "\n",
    "**Objective**: Train and evaluate 6 classification models to predict obesity levels\n",
    "\n",
    "**Author**: BITS WILP M.Tech AIML Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, \n",
    "    recall_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Dataset source: UCI ML Repository / Kaggle\n",
    "# URL: https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition\n",
    "\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {df.shape[1] - 1}\")\n",
    "print(f\"Instances: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"\\nTarget Variable Distribution (NObeyesdad):\")\n",
    "print(\"=\"*50)\n",
    "print(df['NObeyesdad'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['NObeyesdad'].value_counts().plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Distribution of Obesity Levels', fontsize=14)\n",
    "plt.xlabel('Obesity Level', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical Columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Label encode categorical variables\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "print(\"\\nLabel encoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('NObeyesdad', axis=1)\n",
    "y = df_processed['NObeyesdad']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "### Evaluation Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model and return all required metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # AUC Score (multi-class: one-vs-rest)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            auc = 0.0\n",
    "    else:\n",
    "        auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'AUC': round(auc, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1': round(f1, 4),\n",
    "        'MCC': round(mcc, 4)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "models_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_metrics, lr_pred, lr_cm = evaluate_model(lr_model, X_test_scaled, y_test, 'Logistic Regression')\n",
    "results.append(lr_metrics)\n",
    "models_dict['Logistic Regression'] = lr_model\n",
    "\n",
    "print(f\"\\nLogistic Regression Results:\")\n",
    "for key, value in lr_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Decision Tree Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_metrics, dt_pred, dt_cm = evaluate_model(dt_model, X_test_scaled, y_test, 'Decision Tree')\n",
    "results.append(dt_metrics)\n",
    "models_dict['Decision Tree'] = dt_model\n",
    "\n",
    "print(f\"\\nDecision Tree Results:\")\n",
    "for key, value in dt_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Decision Tree - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training K-Nearest Neighbors Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_metrics, knn_pred, knn_cm = evaluate_model(knn_model, X_test_scaled, y_test, 'KNN')\n",
    "results.append(knn_metrics)\n",
    "models_dict['KNN'] = knn_model\n",
    "\n",
    "print(f\"\\nKNN Results:\")\n",
    "for key, value in knn_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(knn_cm, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title('KNN - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Naive Bayes Classifier (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Gaussian Naive Bayes Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "nb_metrics, nb_pred, nb_cm = evaluate_model(nb_model, X_test_scaled, y_test, 'Naive Bayes')\n",
    "results.append(nb_metrics)\n",
    "models_dict['Naive Bayes'] = nb_model\n",
    "\n",
    "print(f\"\\nNaive Bayes Results:\")\n",
    "for key, value in nb_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Purples')\n",
    "plt.title('Naive Bayes - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Random Forest (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=15)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_metrics, rf_pred, rf_cm = evaluate_model(rf_model, X_test_scaled, y_test, 'Random Forest')\n",
    "results.append(rf_metrics)\n",
    "models_dict['Random Forest'] = rf_model\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "for key, value in rf_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='YlGn')\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 XGBoost (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost Classifier...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "xgb_metrics, xgb_pred, xgb_cm = evaluate_model(xgb_model, X_test_scaled, y_test, 'XGBoost')\n",
    "results.append(xgb_metrics)\n",
    "models_dict['XGBoost'] = xgb_model\n",
    "\n",
    "print(f\"\\nXGBoost Results:\")\n",
    "for key, value in xgb_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='RdYlGn')\n",
    "plt.title('XGBoost - Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "metrics_to_plot = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'orchid', 'goldenrod', 'slategray']\n",
    "\n",
    "for idx, (metric, ax) in enumerate(zip(metrics_to_plot, axes.flat)):\n",
    "    values = results_df[metric].values\n",
    "    models = results_df.index.tolist()\n",
    "    \n",
    "    bars = ax.bar(models, values, color=colors[idx], edgecolor='black')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model identification\n",
    "best_accuracy_model = results_df['Accuracy'].idxmax()\n",
    "best_f1_model = results_df['F1'].idxmax()\n",
    "best_auc_model = results_df['AUC'].idxmax()\n",
    "\n",
    "print(\"\\nBest Performing Models:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Accuracy: {best_accuracy_model} ({results_df.loc[best_accuracy_model, 'Accuracy']})\")\n",
    "print(f\"Best F1 Score: {best_f1_model} ({results_df.loc[best_f1_model, 'F1']})\")\n",
    "print(f\"Best AUC Score: {best_auc_model} ({results_df.loc[best_auc_model, 'AUC']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saved models\n",
    "os.makedirs('trained_models', exist_ok=True)\n",
    "\n",
    "# Save all models\n",
    "for name, model in models_dict.items():\n",
    "    filename = f\"trained_models/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# Save scaler and label encoders\n",
    "joblib.dump(scaler, 'trained_models/scaler.pkl')\n",
    "joblib.dump(label_encoders, 'trained_models/label_encoders.pkl')\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X.columns)\n",
    "joblib.dump(feature_names, 'trained_models/feature_names.pkl')\n",
    "\n",
    "# Save class labels\n",
    "class_labels = label_encoders['NObeyesdad'].classes_.tolist()\n",
    "joblib.dump(class_labels, 'trained_models/class_labels.pkl')\n",
    "\n",
    "# Save results DataFrame\n",
    "results_df.to_csv('trained_models/model_results.csv')\n",
    "\n",
    "# Save test data for Streamlit app\n",
    "test_data = X_test.copy()\n",
    "test_data['NObeyesdad'] = label_encoders['NObeyesdad'].inverse_transform(y_test)\n",
    "test_data.to_csv('trained_models/test_data.csv', index=False)\n",
    "\n",
    "print(\"\\nAll models and artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_markdown())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Observations\n",
    "\n",
    "| ML Model Name | Observation about model performance |\n",
    "|---------------|------------------------------------|\n",
    "| **Logistic Regression** | Provides a solid baseline with good interpretability. Performance is moderate due to the linear decision boundary assumption, which may not capture complex non-linear relationships in the obesity data. Works well when features have linear relationships with the target. |\n",
    "| **Decision Tree** | Shows good performance with ability to capture non-linear patterns. May be prone to overfitting without proper depth constraints. Provides excellent interpretability through feature importance and tree visualization. |\n",
    "| **KNN** | Performance depends heavily on the choice of k and feature scaling. Works well when similar obesity levels cluster together in feature space. Can be computationally expensive for large datasets during prediction. |\n",
    "| **Naive Bayes** | Assumes feature independence which may not hold for obesity data (e.g., height and weight are correlated). Fastest training time and works well with limited data. Lower performance compared to other models due to independence assumption violation. |\n",
    "| **Random Forest (Ensemble)** | Excellent performance due to ensemble averaging that reduces overfitting. Robust to outliers and handles mixed feature types well. Feature importance provides insights into key obesity predictors like weight, height, and physical activity. |\n",
    "| **XGBoost (Ensemble)** | Top performer with highest accuracy and AUC. Sequential boosting effectively handles complex feature interactions. Regularization prevents overfitting. Best suited for this multi-class obesity classification task. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Files saved in 'trained_models/' directory:\")\n",
    "for f in os.listdir('trained_models'):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
